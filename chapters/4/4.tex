\providecommand{\main}{../../main}
\providecommand{\figpath}[1]{\main/../chapters/#1}
\documentclass[../../main/main.tex]{subfiles}



\begin{document}

\chapter{The bias complexity trade-off}

Let \( A \) be the learning algorithm, \( S \) the training set, \( \mathcal{H} \) the hypothesis class and \( \hat{h} \in \mathcal{H} \) such that \( L_{\mathcal{D}}(\hat{h}) \) is small. What we want to know is if there is a universal learner, i.e. an algorithm \( A \) that predicts the best \( \hat{h} \) for any distribution \( \mathcal{D} \).





\section{No-Free-Lunch theorem}
\begin{theorem}[No-Free-Lunch]
    Let \( A \) be any learning algorithm for the task of binary classification with respect to the 0-1 loss over a domain \( \mathcal{X} \).
    Let \( m \) be any number smaller than \( \frac{\abs{\mathcal{X}}}{2} \), representing a training set size.
    Then, there exists a distribution \( \mathcal{D} \) over \( \mathcal{X} \times \{0,1\} \) such that:
    \begin{itemize}
        \item There exists a function \( f \ : \ \mathcal{X} \to \{0,1\} \) with \( L_{\mathcal{D}}(f) = 0 \).
        \item With probability of at least \( \frac{1}{7} \) over the choice of \( S \sim \mathcal{D}^m \) we have that \( L_{\mathcal{D}}(A(S)) \ge \frac{1}{8} \).
    \end{itemize}
\end{theorem}

The meaning of this theorem is clear: a universal learner can't exist. In fact, no learner can succeed on all learning tasks. FIn other words, for every learner, there exists a task on which it fails, even though that task can be successfully learned by another learner.

\begin{corollary}[]
    Let \( \mathcal{X} \) be an infinite domain set and let \( \mathcal{H} \) be the set of all functions from \( \mathcal{X} \) to \( \{0,1\} \). Then, \( \mathcal{H} \) is not PAC learnable.
\end{corollary}

Firstly, we give the idea of the proof. Our training set is smaller than half of the domain, so we have no information on what happens on the other half. There exists some target function \( f \) that works on the other half in a way that contradicts our estimated labels.

\begin{remark}
    A class \( \mathcal{H} \) of all possible functions from \( \mathcal{X} \) to \( \{0,1\} \) (i.e. assuming no prior knowledge) is not a good idea since ii isn't PAC learnable.
\end{remark}

\begin{proof}
    Let's demonstate the previous corollary. We proceed by contradiction:
    \begin{enumerate}
        \item Assume PAC learnability:
            \begin{itemize}
                \item We choose \( \varepsilon < \frac{1}{8} \), \( \delta < \frac{1}{7} \) (Note that \( \mathcal{H} \) includes all functions).
                \item By definition of PAC: there exists an algorithm \( A \) such that for very distribution \( \mathcal{D} \), if realizable, when running the algorithm on \( m \) i.i.d. examples generated by \( \mathcal{D} \), the algorithm returns a hypothesis \( h \) such that, with probability \( \ge 1 - \delta \), \( L_{\mathcal{D}}(A(S)) \le \varepsilon \).
            \end{itemize}

        \item Apply No-Free-Lunch theorem to \( A \):
            \begin{itemize}
                \item \( \abs{\mathcal{X}} > 2m \): for any ML algorithm (including \( A \)), there exists a distribution \( \mathcal{D} \) for which with probability \( \ge \frac{1}{7} > \delta \), \( L_{\mathcal{D}}(A_S) \ge \frac{1}{8} > \varepsilon \).
            \end{itemize}

        \item Contradiction!
    \end{enumerate}
\end{proof}

\begin{remark}
    We want to choose a good hypotesis set:
    \begin{itemize}
        \item We need to use our prior knowledge about \( \mathcal{D} \) to pick a good hypothesis set.
        \item We would like \( \mathcal{H} \) to be large, so that it may contain a function \( h \) with small \( L_S(h) \) and hopefully a small \( L_{\mathcal{D}}(h) \).
        \item No-Free-Lunch theorem tells us that \( \mathcal{H} \) can't be too large! In fact:
        \begin{itemize}
            \item[$\triangleright$] \( \mathcal{H} \) too small \( \Longrightarrow \) large \( L_S \) and \( L_{\mathcal{D}} \sim L_S \).
            \item[$\triangleright$] \( \mathcal{H} \) too large \( \Longrightarrow \) small \( L_S \) and \( L_{\mathcal{D}} \gg L_S \) (risk of overfitting).
        \end{itemize}
    \end{itemize}
\end{remark}





\section{Error decomposition}
To have a more clear vision, we decompose the error of an ERM$_{\mathcal{H}}$ predictor into two components as follows. Let \( h_S \) be an ERM$_{\mathcal{H}}$ hypotesis. Then, we can write:

\begin{equation}
    L_{\mathcal{D}}(h_S) = \varepsilon_{\text{app}} + \varepsilon_{\text{est}}
    \label{eq:C4_ED}
\end{equation}

where:

\begin{subequations}
    \begin{align}
        \varepsilon_{\text{app}} &= \underset{h \in \mathcal{H}}{\min{}} L_{\mathcal{D}}(h) \label{eq:C4_EDAE}   \\
        \varepsilon_{\text{est}} &= L_{\mathcal{D}}(h_S) - \varepsilon_{\text{app}}         \label{eq:C4_EDEE}
    \end{align}
\end{subequations}

\begin{itemize}
    \item \textbf{Approximation Error}:\\
        It is the minimum risk achievable by a predictor in the hypotesis class. This term measures how much risk we have because we restrict ourselves to a specific class, namely how much \textbf{inductive bias} we have. Other informations on \( \varepsilon_{\text{app}} \):
        \begin{itemize}
            \item[$\triangleright$] It depends on the choice of \( \mathcal{H} \).
            \item[$\triangleright$] It doesn't depend on the sample size \( m \).
            \item[$\triangleright$] Under the realizability assumption: \( \varepsilon_{\text{app}} = 0 \)
            \item[$\triangleright$] The larger \( \mathcal{H} \) is, the smaller \( \varepsilon_{\text{app}} = 0 \) is.
        \end{itemize}

    \item \textbf{Estimation Error}:\\
        It is the difference between the approximation error and the error achieved by the ERM predictor. It results because the empirical risk (i.e. the training error) is only an estimate of the true risk and so the predictor minimizing the empirical risk is only an estimate of the predictor minimizing the true risk. Other informations on \( \varepsilon_{\text{est}} \):
        \begin{itemize}
            \item[$\triangleright$] It depends on the training set size \( m \) and on the size (complexity) of the hypothesis class.
            \item[$\triangleright$] \( \varepsilon_{\text{est}} \) increases with \( \abs{\mathcal{H}} \) and decreases with \( m \).
        \end{itemize}
\end{itemize}

Since our goal is to minimize the total risk, we face a tradeoff, called the \textbf{bias-complexity tradeoff}:
\begin{itemize}
    \item Choosing \( \mathcal{H} \) large \( \Longrightarrow \) decreases \( \varepsilon_{\text{app}} \), increases \( \varepsilon_{\text{est}} \). This leads to overfitting.
    \item Choosing \( \mathcal{H} \) small \( \Longrightarrow \) decreases \( \varepsilon_{\text{est}} \), increases \( \varepsilon_{\text{app}} \). This leads to underfitting.
\end{itemize}





\section{Train, test and validation sets}
In most practical applications, the available set of examples is split into more sets with different purposes. In order:
\begin{itemize}
    \item We estimate \( L_{\mathcal{D}}(h) \) (with \( h \) selected with ERM).

    \item We extract a \textbf{test set} from the original one, namely a new set of samples not used for picking \( h \). It is different from the training set and it leads to a more reliable estimation of \( L_{\mathcal{D}}(h) \). We must not look to the test set until we have picked our final hypothesis. In practice, one set of sample is splitted into training set and test set.

    \item Sometimes the training set is divided into training set and \textbf{validation set}. The ladder is used for selecting the hyperparameters of the algorithm.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{\figpath{4}/images/C4_TVTS.pdf}
    \caption{Scheme of the division of the set of examples \( S \).}
    \label{fig:C4_TVTS}
\end{figure}

Hence, the training routine is:
\begin{itemize}
    \item Train model on training set.
    \item Evaluate model on validation set.
    \item Tweak model according to the results on validation set. Restart from the first point.
    \item When the learning procedure has finished due to a particular ending condition, pick the model that performs better on validation set.
    \item Confirm the result on test set.
\end{itemize}


\end{document}
