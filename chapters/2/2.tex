\providecommand{\main}{../../main}
\providecommand{\figpath}[1]{\main/../chapters/#1}
\documentclass[../../main/main.tex]{subfiles}



\begin{document}

\chapter{A formal learning model}

\section{Probably Approximated Correct (PAC) learning}
\begin{definition}[PAC learnability]
    A hypothesis class \( \mathcal{H} \) is PAC learnable if there exist a function \( m_{\mathcal{H}} \ : \ (0,1)^2 \to \mathbb{N} \) and a learning algorithm with the following property:
    \( \forall \varepsilon, \delta \in (0,1) \), for every distribution \( \mathcal{D} \) over \( \mathcal{X} \),
    and for every labeling function \( f \ : \ \mathcal{X} \to \{ 0,1 \} \), if the realizable assumption holds with respect to \( \mathcal{H}, \mathcal{D}, f \),
    then when running the learning algorithm on \( m \ge m_{\mathcal{H}}(\varepsilon, \delta) \) i.i.d. examples generated by \( \mathcal{D} \) and labeled by \( f \),
    the algorithm returns a hypotesis \( h \) such that, with probability of at least \( 1 - \delta \) (over the choice of the examples), \( L_{(\mathcal{D},f)}(h) \le \varepsilon \).
\end{definition}

The function \( m_{\mathcal{H}} \ : \ (0,1)^2 \to \mathbb{N} \) determines the sample complexity of learning \( \mathcal{H} \).
So it determines how many samples are required to guarantee a probably approximately correct solution.

\begin{corollary}[]
    Every finite class is PAC learnable with sample complexity:
    \begin{equation}
        m_{\mathcal{H}}(\varepsilon, \delta)
        \le
        \left[
        \frac{1}{\varepsilon} \log{\left( \frac{\abs{\mathcal{H}}}{\delta} \right)}
        \right]
        \label{eq:C2_PAC}
    \end{equation}
\end{corollary}

Now we want to generalize by:
\begin{itemize}
    \item removing the realizability assumption;
    \item extending to learning problems beyond the simple binary classification.
\end{itemize}





\section{Agnostic PAC learning}




\end{document}
