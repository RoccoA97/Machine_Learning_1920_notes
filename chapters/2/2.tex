\providecommand{\main}{../../main}
\providecommand{\figpath}[1]{\main/../chapters/#1}
\documentclass[../../main/main.tex]{subfiles}



\begin{document}

\chapter{A formal learning model}

\section{Probably Approximated Correct (PAC) learning}

\begin{definition}[PAC learnability]
    A hypothesis class \( \mathcal{H} \) is PAC learnable if there exist a function \( m_{\mathcal{H}} \ : \ (0,1)^2 \to \mathbb{N} \) and a learning algorithm with the following property:
    \( \forall \varepsilon, \delta \in (0,1) \), for every distribution \( \mathcal{D} \) over \( \mathcal{X} \),
    and for every labeling function \( f \ : \ \mathcal{X} \to \{ 0,1 \} \), if the realizable assumption holds with respect to \( \mathcal{H}, \mathcal{D}, f \),
    then when running the learning algorithm on \( m \ge m_{\mathcal{H}}(\varepsilon, \delta) \) i.i.d. examples generated by \( \mathcal{D} \) and labeled by \( f \),
    the algorithm returns a hypotesis \( h \) such that, with probability of at least \( 1 - \delta \) (over the choice of the examples), \( L_{(\mathcal{D},f)}(h) \le \varepsilon \).
\end{definition}

The function \( m_{\mathcal{H}} \ : \ (0,1)^2 \to \mathbb{N} \) determines the sample complexity of learning \( \mathcal{H} \).
So it determines how many samples are required to guarantee a probably approximately correct solution.

\begin{corollary}[]
    Every finite class is PAC learnable with sample complexity:
    \begin{equation}
        m_{\mathcal{H}}(\varepsilon, \delta)
        \le
        \left[
        \frac{1}{\varepsilon} \log{\left( \frac{\abs{\mathcal{H}}}{\delta} \right)}
        \right]
        \label{eq:C2_PAC}
    \end{equation}
\end{corollary}

Now we want to generalize by:
\begin{itemize}
    \item Removing the realizability assumption.\\
        For practical learning tasks, this assumption may be too strong and it isn't necessarily met.
    \item Extending to learning problems beyond the simple binary classification.\\
        One may wish to predict real valued numbers or a label picked from a finite set of labels. Therefore our analysis can be extended to many othere scenarios by allowing a variety of loss functions.
\end{itemize}





\section{Agnostic PAC learning}
We introduce a more realistic model for the data-generating distribution. From now on, let \( \mathcal{D} \) be a probability distribution over \( \mathcal{X} \times \mathcal{Y} \) (the meaning of the notation hasn't changed). \( \mathcal{D} \) is a \textbf{joint distribution} over domain points and labels and it can be viewed as the composition of two parts:
\begin{itemize}
    \item A distribution \( \mathcal{D}_x \) over unlabeled domain points (also called \textbf{marginal distribution}).
    \item A \textbf{conditional probability} \( \mathcal{D}((x,y)|x) \) over labels for each domain point.
\end{itemize}



\subsection{Empirical and true error revised}
One can measure how likely \( h \) is to make an error when labeled points are randomly drawn according to \( \mathcal{D} \) previously introduced. So we redefine the true error of a prediction rule \( h \):

\begin{equation}
    L_{\mathcal{D}}(h)
    :=
    \underset{(x,y) \sim \mathcal{D}}{\mathbb{P}}[h(x) \neq y]
    :=
    \mathcal{D}(\{ (x,y) \ : \ h(x) \neq y \})
    \label{eq:C2_TER}
\end{equation}

We would like to find a predictor \( h \) for which the error in Eq. \ref{eq:C2_TER} will be minimized. However, the learner doesn't know the data generating \( \mathcal{D} \), but he has access only to the training set \( S \). Hence the definition of the empirical risk remains the same as before:

\begin{equation}
    L_S (h)
    :=
    \frac{\abs{\{ i \in [m] \ : \ h(x_i) \neq y_i \}} }{m}
    \label{eq:C2_ER}
\end{equation}

The goal is to find some hypotesis \( h \ : \ \mathcal{X} \to \mathcal{Y} \) that (probably approximately) minimizes the true risk, \( L_{\mathcal{D}}(h) \).



\subsection{Bayes optimal predictor}
Given any probability distribution \( \mathcal{D} \) over \( \mathcal{X} \times \{0,1\} \), the best label predicting function from \( \mathcal{X} \) to \( \{0,1\} \) will be:

\begin{equation}
    f_{\mathcal{D}}(x)
    =
    \begin{cases}
        1   &   \text{if } \mathbb{P}[y=1|x] \ge \frac{1}{2}    \\
        0   &   \text{otherwise}
    \end{cases}
    \label{eq:C2_BOP}
\end{equation}

\begin{proposition}
    For every probability distribution \( \mathcal{D} \), the Bayes optimal predictor \( f_{\mathcal{D}} \) is optimal, in the sense that no other classifier \( g \ : \ \mathcal{X} \to \{0,1\} \) has a lower error. Mathematically:

    \begin{equation}
        L_{\mathcal{D}}(f_{\mathcal{D}}) \le L_{\mathcal{D}}(g)
        \qquad
        \forall g
        \label{eq:C2_BOPB}
    \end{equation}
\end{proposition}

Clearly, we can't hope that the learning algorithm will find a hypotesis whose error is smaller than the minimal possible error of the Bayes predictor. We require that the learning algorithm will find a predictor whose error is not much larger than the best possible error of a predictor in some given benchmark hypotesis class. So, we generalize the definition of PAC learning.

\begin{definition}[Agnostic PAC learnability]
    A hypotesis class is PAC learnable if there exist a function \( m_{\mathcal{H}} \ : \ (0,1)^2 \to \mathbb{N} \) and a learning algorithm with the following property:
    for every \( \varepsilon, \delta \in (0,1) \) and
    for every distribution \( \mathcal{D} \) over \( \mathcal{X} \times \mathcal{Y} \),
    when running the learning algorithm on \( m \ge m_{\mathcal{H}}(\varepsilon,\delta) \) i.i.d. examples generated by \( \mathcal{D} \),
    the algorithm returns a hypotesis \( h \) such that, with probability of at least \( 1 - \delta \) (over the choice of the \( m \) training examples):

    \begin{equation}
        L_{\mathcal{D}}(h)
        \le
        \underset{h' \in \mathcal{H}}{\min{}}L_{\mathcal{D}}(h') + \varepsilon
        \label{eq:C2_APL}
    \end{equation}
\end{definition}

In the agnostic case, the learner is required to achieve a small error relative to the best error achievable by the hypotesis class and not in absolute terms. In other words, we drop the requirement of finding the best predictor, but we don't want to be too far from it.



\subsection{Scope of the learning problems}
We want to extend our model so that it can be applied to a wide variety of learning tasks, such as:
\begin{itemize}
    \item \textbf{Multiclass Classification}:\\
        Our classification doesn't have to be binary. The label set can be much more complex, for example it can be a set of real numbers.

    \item \textbf{Regression}:11
        In this task, one wishes to find some simple pattern in the data, i.e. a functional relationship between the \( \mathcal{X} \) and \( \mathcal{Y} \) components of the data. For example, one wishes to find a linear function that describes the correlation between a sample of \( \mathcal{X} \) and the corresponding value inside \( \mathcal{Y} \). However, for this type of tasks, the measure of success has to be changed. We may evaluate the quality of a hypotesis function, \( h \ : \ \mathcal{X} \to \mathcal{Y} \), by the \emph{expected square difference} between the true labels and their predicted values, namely:

        \begin{equation}
            L_{\mathcal{D}}(h) := \underset{(x,y) \sim \mathcal{D}}{\mathbb{E}} [h(x) - y]^2
            \label{eq:C2_ESD}
        \end{equation}
\end{itemize}

To extend the reach of machine learning techniques we must generalize our formalism of the measure of success.



\subsection{Generalized loss functions and common examples}
Given any set \( \mathcal{H} \) (our hypotheses or models) and some domain \( Z \), let \( \ell \) be any function from \( \mathcal{H} \times Z \) to the set of non-negative real numbers, \( \ell \), namely \( \mathcal{H} \times z \to \mathbb{R}_{+} \). We call such functions \textbf{loss functions}.

We now define the \textbf{risk function} to be the expected loss of a classifier \( h \in \mathcal{H} \) with respect to a probability distribution \( \mathcal{D} \) over \( Z \), namely:

\begin{equation}
    L_{\mathcal{D}}(h) := \underset{z \sim \mathcal{D}}{\mathbb{E}} [\ell (h,z)]
    \label{eq:C2_RF}
\end{equation}

In other words, we consider the expectation of the loss of \( h \) over objects \( z \) picked randomly according to \( \mathcal{D} \). Similarly, we define the \textbf{empirical risk} to be the expected loss over a given sample \( S = (z_1, \dots, z_m) \in Z^m \), namely:

\begin{equation}
    L_S (h) := \frac{1}{m} \sum_{i=1}^{m} l(h,z_i)
    \label{eq:C2_ERG}
\end{equation}

Some of the most common examples of losses are given below:
\begin{itemize}
    \item \textbf{0-1 loss}:\\
        Our random variable \( z \) ranges over the set of pairs \( \mathcal{X} \times \mathcal{Y} \) and the loss function is:

        \begin{equation}
            \ell_{0-1}(h,(x,y)) :=
            \begin{cases}
                0   &   \text{if } h(x) = y \\
                1   &   \text{if } h(x) \neq y
            \end{cases}
            \label{eq:C2_01L}
        \end{equation}

        It is used in binary and multiclass classification.

    \item \textbf{Square loss (L2)}:\\
        Our loss function is:

        \begin{equation}
            \ell_\mathrm{sq}(h,(x,y)) := [h(x) - y]^2
            \label{eq:C2_SL}
        \end{equation}

        This loss is used in regression problems to penalize few large errors.

    \item \textbf{Absolute value loss (L1)}:\\
        Just like before, but with modulus and not the square:

        \begin{equation}
            \ell_\mathrm{abs}(h,(x,y)) := \abs{h(x) - y}
            \label{eq:C2_AVL}
        \end{equation}

        It is used in regression tasks to penalize many small errors.
\end{itemize}

To summarize, we formally define agnostic PAC learnability for general loss functions.

\begin{definition}[Agnostic PAC learnability for general loss functions]
    A hypotesis class \( \mathcal{H} \) is PAC learnable with respect to a set \( Z \) and a loss function \( \ell \ : \ \mathcal{H} \times Z \to \mathbb{R}_{+} \), if there exist a function \( m_{\mathcal{H}} \ : \ (0,1)^2 \to \mathbb{N} \) and a learning algorithm with the following property:
    for every \( \varepsilon, \delta \in (0,1) \) and for every distribution \( \mathcal{D} \) over \( Z \), when running the learning algorithm on \( m \ge m_{\mathcal{H}}(\varepsilon, \delta) \) i.i.d. examples generated by \( \mathcal{D} \), the algorithm returns \( h \in \mathcal{H} \) such that, with probability of at least \( 1 - \delta \) (over the choice of the \( m \) training examples):

    \begin{equation}
        l_{\mathcal{D}}(h)
        \le
        \underset{h' \in \mathcal{H}}{\min{}} L_{\mathcal{D}}(h') + \varepsilon
        \label{eq:C2_APLG}
    \end{equation}

    where \( L_{\mathcal{D}}(h) = \mathbb{E}_{z \sim \mathcal{D}}[\ell (h,z)] \)
\end{definition}

\begin{remark}
    We can consider \( \ell (h,\bullet) \ : \ Z \to \mathbb{R}_{+} \) a random variable. Hence \( L_{\mathcal{D}}(h) \) is the expected value of this random variable.
\end{remark}

\end{document}
