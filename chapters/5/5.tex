\providecommand{\main}{../../main}
\providecommand{\figpath}[1]{\main/../chapters/#1}
\documentclass[../../main/main.tex]{subfiles}



\begin{document}

\chapter{Linear predictors}
In this Chapter we will study the family of linear predictors, one of the most useful families of hypothesis classes. Many learning algorithms that are being widely used in practice rely on linear predictors, for their ability to learn them efficiently in many cases and because they are intuitive and easy to interpret.

In particular, we focus on learning linear predictors using the ERM approach. The hypothesis classes can include:
\begin{itemize}
    \item Halfspaces (classification).
    \item Linear regression (regression).
    \item Logistic regression (classification moduled as a regression problem).
\end{itemize}

The algorithms can include:
\begin{itemize}
    \item Linear programming (for halfspaces).
    \item Perceptron (for halfspaces).
    \item Least squares (for regression).
\end{itemize}

First, we define the class of \textbf{affine functions} as:

\begin{equation}
    L_d = \{ h_{\mathbf{w},b} \ : \ \mathbf{w} \in \mathbb{R}^d, \ b \in \mathbb{R} \}
    \label{eq:C5_AF}
\end{equation}

where:

\begin{equation}
    h_{\mathbf{w},b} (\mathbf{x})
    =
    \left\langle \mathbf{w},\mathbf{x} \right\rangle + b
    =
    \left( \sum_{i=1}^{d} w_i x_i \right) + b
    \label{eq:C5_AFH}
\end{equation}

It is convenient to use the following notation for \( L_d \):

\begin{equation}
    L_d = \{ \mathbf{x} \mapsto \left\langle \mathbf{w}, \mathbf{x} \right\rangle + b \ : \ \mathbf{w} \in \mathbb{R}^d, \ b \in \mathbb{R} \}
    \label{eq:C5_AF_2}
\end{equation}

which reads as follows: \( L_d \) is a set of functions, where each function is parametrized by \( \mathbf{w} \in \mathbb{R}^d \) and \( b \in \mathbb{R} \), and each such function takes as input a vector \( \mathbf{x} \) and returns as output the scalar \( \left\langle \mathbf{w}, \mathbf{x} \right\rangle + b \). Therefore the dimension of the parameter space increases from \( d \) to \( d+1 \).

The different hypothesis classes of linear predictors are composition of function \( \varphi \ : \ \mathbb{R} \to \mathcal{Y} \) on \( L_d \). For example:
\begin{itemize}
    \item For binary classification: \( \mathcal{Y} = \{-1,1\} \to \varphi(z) = \operatorname{sign}(z) \)
    \item For regression: \( \mathcal{Y} = \mathbb{R} \to \varphi(z) = z \)
\end{itemize}

\begin{equation}
    \begin{aligned}
        \mathbf{w}' &= (b, w_1, w_2, \dots, w_d) \in \mathbb{R}^{d+1}   \\
        \mathbf{x}' &= (1, x_1, x_2, \dots, x_d) \in \mathbb{R}^{d+1}
    \end{aligned}
    \label{eq:C5_AFNN}
\end{equation}

Therefore \( h_{\mathbf{w},b}(\mathbf{x}) \) becomes a linear function:

\begin{equation}
    h_{\mathbf{w},b}(\mathbf{x})
    =
    \left\langle \mathbf{w}, \mathbf{x} \right\rangle + b
    =
    \left\langle \mathbf{w}', \mathbf{x}' \right\rangle
    \label{eq:C5_AFNN_2}
\end{equation}





\section{Halfspaces}
They are exploited for binary classification problems, namely \( \mathcal{X} = \mathbb{R}^d \) and \( \mathcal{Y} = \{-1,+1\} \). The class of halfspaces is defined as follows:

\begin{equation}
    H S_d = \operatorname{sign} \circ L_d
    =
    \{ \mathbf{x} \mapsto \operatorname{sign}(h_{\mathbf{w},b}(\mathbf{x})) \ : \ h_{\mathbf{w},b} \in L_d \}
    \label{eq:C5_CH}
\end{equation}

In other words, each halfspace hypothesis in \( HS_d \) is parametrized by \( \mathbf{w} \in \mathbb{R}^d \) and \( b \in \mathbb{R} \) and upon receiving a vector \( \mathbf{x} \) the hypotesis returns the label sign \( (\left\langle \mathbf{w}, \mathbf{x} \right\rangle + b) \).

The realizability can be obtained only if the space is linearly separable.

To illustrate this hypothesis class geometrically, let's consider the case \( d=2 \). Each hypothesis forms a hyperplane that is perpendicular to the vector \( \mathbf{w} \) and intersects the vertical axis at the point \( (0, - \frac{b}{w_2}) \). The istances that are ``above'' the hyperplane, that is, share an acute angle with \( \mathbf{w} \), are labeled positively. Istances that are ``below'' the hyperplane, that is, share an obtuse angle with \( \mathbf{w} \), are labeled negatively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\textwidth]{\figpath{5}/images/C5_HG.pdf}
    \caption{Example of halfspace with \( d=2 \).}
    \label{fig:C5_HG}
\end{figure}

From Fig. \ref{fig:C5_HG} it is clear the condition needed for the realizability. Implementing the ERM rule in non-separable case (i.e. in the agnostic case) is known to be computationally hard, but there are several approaches to tackle problems with non-separable data.



\subsection{Linear programming for the class of halfspaces}
Linear programs (LP) are problems that can be expressed as maximizing a linear function subject to linear inequalities (constraints). That is:

\begin{equation}
    \underset{\mathbf{w} \in \mathbb{R}^d}{\max{}} \left\langle \mathbf{u}, \mathbf{w} \right\rangle
    \qquad
    \text{subject to } A \mathbf{w} \ge \mathbf{v}
    \label{eq:C5_LPD}
\end{equation}

where \( \mathbf{w} \in \mathbb{R}^d \) is the vector of variables we wish to determine, \( A \) is an \( m \times d \) matrix, and \( \mathbf{v} \in \mathbb{R}^m \), \( \mathbf{u} \in \mathbb{R}^d \) are vectors. Linear programming can be solved efficiently\footnotemark, and furthermore, there are publicly implementations of LP solvers.
\footnotetext{Namely, in time polynomial in \( m, d \), and in the representation size of real numbers.}

The ERM problem for halfspaces in the realizable case can be expressed as a linear program. To show this, we assume for simplicity the homogeneous case. Let \( S = \{ (\mathbf{x}_i, y_i) \}_{i=1}^{m} \) be a training set of size \( m \). We can say:
\begin{itemize}
    \item For the realizable assumption, an ERM predictor should have zero errors on the training set. That is, we are looking for some vector \( \mathbf{w} \in \mathbb{R}^d \) for which:
        \begin{equation}
            y_i \left\langle \mathbf{w}, \mathbf{x}_i \right\rangle > 0
            \qquad
            \forall i = 1, \dots, m
            \label{eq:C5_LPCH}
        \end{equation}

    \item There exists also a vector \( \mathbf{w} \) that satisfy
        \begin{equation}
            y_i \left\langle \mathbf{w}, \mathbf{x}_i \right\rangle \ge 1
            \qquad
            \forall i = 1, \dots, m
            \label{eq:C5_LPCH_2}
        \end{equation}

        \begin{proof}
            Let \( \mathbf{w}^* \) be a vector that satisfies \( y_i \left\langle \mathbf{w}, \mathbf{x}_i \right\rangle > 0 \ \forall i \).
            We define \( \gamma = \min{}_i(y_i \left\langle \mathbf{w}^*, \mathbf{x}_i \right\rangle) \) and let \( \bar{\mathbf{w}} = \frac{\mathbf{w}^*}{\gamma} \).
            Therefore, \( \forall i \) we have:

            \[
                y_i \left\langle \bar{\mathbf{w}}, \mathbf{x}_i \right\rangle
                =
                \frac{1}{\gamma} y_i \left\langle \mathbf{w}^*, \mathbf{x}_i \right\rangle
                \ge
                1
            \]

            We have thus shown that there exists a vector that satisfies Eq. \ref{eq:C5_LPCH_2}.
        \end{proof}

    \item We want to express the ERM model with linear programming.
        Let \( A \) be a \( m \times d \) matrix such that \( A_{ij} = y_i x_{ij} \), with \( x_ij \) the \( j^{\text{th}} \) element of the vector \( \mathbf{x}_i \).
        Let \( \mathbf{v} \) be the vector \( (1,\dots,1) \in \mathbb{R}^m \). Then, Eq. \ref{eq:C5_LPCH_2} can be rewritten as:

        \[
            A \mathbf{w} \ge \mathbf{v}
        \]

        The LP form requires a maximization objective, yet all the \( \mathbf{w} \) that satisfy the constraints are equal candidates as output hypotheses. Thus, we set a ``dummy'' objective \( \mathbf{u} = (0,\dots,0) \in \mathbb{R}^d \).
\end{itemize}



\subsection{Perceptron for halfspaces}
It is a different implementation of the ERM rule by Rosenblatt (1958). The perceptron is an iterative algorithm that constructs a sequence of vectors \( \mathbf{w}^{(1)} \), \( \mathbf{w}^{(2)} \), \( \dots \).
Initially, \( \mathbf{w}^{(1)} \) is set to be the all-zeros vector.
At iteration time \( t \), the perceptron finds an example \( i \) that is mislabeled by \( \mathbf{w}^{(t)} \), namely, an example for which \( \operatorname{sign}(\left\langle \mathbf{w}^{(t)}, \mathbf{x}_i \right\rangle) \neq y_i \).
Then, the perceptron updates \( \mathbf{w}^{(t)} \) by adding to it the istance \( \mathbf{x}_i \) scaled by the label \( y_i \). That is:

\[
    \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + y_i \mathbf{x}_i
\]

Our goal is to have \( y_i \left\langle \mathbf{w}, \mathbf{x}_i \right\rangle > 0 \ \forall i \) and note that:

\[
    y_i \left\langle \mathbf{w}^{(t+1)}, \mathbf{x}_i \right\rangle
    =
    y_i \left\langle \mathbf{w}^{(t)} + y_i \mathbf{x}_i, \mathbf{x}_i \right\rangle
    =
    y_i \left\langle \mathbf{w}^{(t)}, \mathbf{x}_i \right\rangle + \norm{\mathbf{x}_i}^2
\]

Hence, the update of the perceptron guides the solution to be ``more correct'' on the \( i^{\text{th}} \) example.

\begin{algorithm}[H]
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \SetKwInOut{Initialize}{initialize}

    \Input{A training set \( (\mathbf{x}_1,y_1), \dots, (\mathbf{x}_m,y_m) \)}
    \Output{\( \mathbf{w}^{(t)} \)}
    \Initialize{\( \mathbf{w}^{(1)} = (0, \dots, 0) \)}

    \For{\( t = 1, 2, \dots \)}{
        \If{\( \exists i \ \text{s.t.} \ y_i \left\langle \mathbf{w}^{(t)}, \mathbf{x}_i \right\rangle \le 0 \)}{
            \( \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + y_i \mathbf{x}_i \)
        }
        \Else{
            \Return{\( \mathbf{w}^{(t)} \)}
        }
    }
    \caption{Batch Perceptron.}
    \label{alg:C5_BPA}
\end{algorithm}

The following theorem guarantees that in the realizable case, the algorithm stops with all sample points correctly classified.

\begin{theorem}
    Assume that \( (\mathbf{x}_1,y_1), \dots, (\mathbf{x}_m,y_m) \) is separable,
    let:

    \begin{align*}
        B &= \min{\{ \norm{\mathbf{w}} \ : \ \forall i \in [m], \ y_i \left\langle \mathbf{w}, \mathbf{x}_i \right\rangle \ge 1 \}} \\
        R &= \max{}_i \norm{\mathbf{x}_i}
    \end{align*}

    Then, the perceptron algorithm stops after at most \( (RB)^2 \) iterations, and when it stops it holds that \( \forall i \in [m], \ y_i \left\langle \mathbf{w}^{(t)}, \mathbf{x}_i \right\rangle > 0 \).
\end{theorem}

\begin{proof}
    We give the proof in steps:
    \begin{itemize}
        \item[$\triangleright$] Let's define a vector \( \mathbf{w}^* \) achieving the minimum in \( B \), and \( T \) the number of iterations before stopping.

        \item[$\triangleright$] Consider:

            \begin{equation}
                \frac{\left\langle \mathbf{w}^*, \mathbf{w}^{(t+1)} \right\rangle}{\norm{\mathbf{w}^*} \norm{\mathbf{w}^{(t+1)}}} \le 1
                \label{eq:C5_T1P}
            \end{equation}

        \item[$\triangleright$] We need to demonstate:

            \begin{equation}
                \frac{\sqrt{T}}{RB}
                \le
                \frac{\left\langle \mathbf{w}^*, \mathbf{w}^{(T+1)} \right\rangle}{\norm{\mathbf{w}^*} \norm{\mathbf{w}^{(T+1)}}}
                \le
                1
                \Longrightarrow
                T < (RB)^2
                \label{eq:C5_T1P_2}
            \end{equation}

        \item[$\triangleright$] We divide this step into two parts:
            \begin{itemize}
                \item[a)] Numerator:\\
                    We want to demonstate that: \( \left\langle \mathbf{w}^*, \mathbf{w}^{(t+1)} \right\rangle \ge T \).
                    \begin{itemize}
                        \item[$\bullet$] First iteration: \( \mathbf{w}^{(1)} = (0,\dots,0) \Longrightarrow  \left\langle \mathbf{w}^*, \mathbf{w}^{(1)} \right\rangle = 0 \)

                        \item[$\bullet$] At each step: \( \left\langle \mathbf{w}^*, \mathbf{w}^{(t+1)} \right\rangle - \left\langle \mathbf{w}^*, \mathbf{w}^{(t)} \right\rangle \ge 1 \)

                        \item[$\bullet$] After \( T \) iterations: \( \left\langle \mathbf{w}^*, \mathbf{w}^{(T+1)} \right\rangle \ge T \)\\
                            In fact:

                            \begin{align}
                                \left\langle \mathbf{w}^*, \mathbf{w}^{(T+1)} \right\rangle
                                &=
                                \sum_{t=1}^{T} \left( \left\langle \mathbf{w}^*, \mathbf{w}^{(t+1)} \right\rangle - \left\langle \mathbf{w}^*, \mathbf{w}^{(t)} \right\rangle \right) \nonumber \\
                                &=
                                %\sum_{t=1}^{T} \left( \left\langle \mathbf{w}^*, \mathbf{w}^{(t+1)} - \mathbf{w}^{(t)} \right\rangle \right)
                                %=
                                \sum_{t=1}^{T} \left\langle \mathbf{w}^*, y_i \mathbf{x}_i \right\rangle
                                \ge
                                T
                                \label{eq:C5_T1P_3}
                            \end{align}
                    \end{itemize}

                \item[b)] Denominator:\\
                    We want to demonstate that: \( \norm{\mathbf{w}^*} \norm{\mathbf{w}^{(T+1)}} \le \sqrt{T} RB \).
                    \begin{itemize}
                        \item[$\bullet$] We have:

                            \begin{align}
                                \norm{\mathbf{w}^{t+1}}
                                =
                                \norm{\mathbf{w}^{(t)} + y_i \mathbf{x}_i}^2
                                &=
                                \norm{\mathbf{w}^{(t)}}^2 + 2 y_i \left\langle \mathbf{w}^{(t)}, \mathbf{x}_i \right\rangle + y_i^2 \norm{\mathbf{x}_i}^2   \nonumber   \\
                                &\le
                                \norm{\mathbf{w}^{(t)}}^2 + R^2
                                \label{eq:C5_T1P_4}
                            \end{align}

                            Therefore:

                            \begin{align}
                                \norm{\mathbf{w}^{(T+1)}}
                                &=
                                \sum_{t=1}^{T} \left( \norm{\mathbf{w}^{(t+1)}}^2 - \norm{\mathbf{w}^{(t)}}^2 \right)   \nonumber   \\
                                &=
                                \sum_{t=1}^{T} \left( \norm{\mathbf{w}^{(t)} + y_i \mathbf{x}_i}^2 - \norm{\mathbf{w}^{(t)}}^2 \right)   \nonumber   \\
                                &=
                                \sum_{t=1}^{T} \left( 2 y_i \left\langle \mathbf{w}^{(t)}, \mathbf{x}_i \right\rangle + \norm{\mathbf{x}_i}^2 \right)
                                \le
                                TR^2
                                \label{eq:C5_T1P_5}
                            \end{align}

                    \end{itemize}
            \end{itemize}

        \item[$\triangleright$] Combining the two previous results we obtain:

            \begin{equation}
                \frac{\left\langle \mathbf{w}^*, \mathbf{w}^{(T+1)} \right\rangle}{\norm{\mathbf{w}^*} \norm{\mathbf{w}^{(T+1)}}}
                \ge
                \frac{T}{BR \sqrt{T}}
                =
                \frac{\sqrt{T}}{BR}
                \label{eq:C5_T1P_6}
            \end{equation}

    \end{itemize}
\end{proof}

\begin{remark}
    Convergence is guaranteed and it depends on \( B \), which can be exponential in \( d \).
\end{remark}





\section{Linear regression}




\end{document}
